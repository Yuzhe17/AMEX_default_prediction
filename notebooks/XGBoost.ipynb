{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bc7d0b6",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f174751e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.model_selection import cross_validate, cross_val_score, cross_val_predict, learning_curve,\\\n",
    "train_test_split, GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, OneHotEncoder, FunctionTransformer, LabelEncoder, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import plot_confusion_matrix, classification_report, precision_recall_curve\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, SGDRegressor, SGDClassifier, Ridge, RidgeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.compose import make_column_selector\n",
    "\n",
    "## pipeline stuff\n",
    "\n",
    "from sklearn.pipeline import Pipeline, make_pipeline, make_union\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer, make_column_selector\n",
    "from sklearn import set_config; set_config(display='diagram')\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.metrics import make_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0b1a97",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6dbe622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def_df = pd.read_csv(\"/home/slawa/code/code-rep0/projects/data/defaulter_data_13364.csv\", index_col=[0])\n",
    "# pay_df = pd.read_csv(\"/home/slawa/code/code-rep0/projects/data/payer_data_41940.csv\", index_col=[0])\n",
    "\n",
    "def_df = pd.read_parquet(\"/Users/sjoerddewit/Desktop/Programming/6 Le Wagon Data Science/final_project/defaulter_data_6k_ids_compress.parquet\")\n",
    "pay_df = pd.read_parquet(\"/Users/sjoerddewit/Desktop/Programming/6 Le Wagon Data Science/final_project/payer_data_20k_ids.parquet\")\n",
    "\n",
    "def_df['default'] = 1\n",
    "pay_df['default'] = 0\n",
    "\n",
    "df = pd.concat([def_df, pay_df])\n",
    "\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "y = df['default'].reset_index(drop=True)\n",
    "\n",
    "X = df.drop(columns=['default']).reset_index(drop=True)\n",
    "\n",
    "cat_vars = ['B_30', \n",
    "            'B_38', \n",
    "            'D_114', \n",
    "            'D_116', \n",
    "            'D_117', \n",
    "            'D_120', \n",
    "            'D_126', \n",
    "            'D_63', \n",
    "            'D_64', \n",
    "            'D_66', \n",
    "            'D_68']\n",
    "\n",
    "X_corr = X.corr()\n",
    "\n",
    "X_corr = X_corr.unstack().reset_index() # Unstack correlation matrix \n",
    "X_corr.columns = ['feature_1','feature_2', 'correlation_all'] # rename columns\n",
    "X_corr.sort_values(by=\"correlation_all\",ascending=False, inplace=True) # sort by correlation\n",
    "X_corr = X_corr[X_corr['feature_1'] != X_corr['feature_2']] # Remove self correlation\n",
    "X_corr = X_corr.drop_duplicates(subset='correlation_all')\n",
    "\n",
    "red_features = list(X_corr[abs(X_corr['correlation_all'])>=.95]['feature_1']) ## abs so we also consider the negative corrs\n",
    "\n",
    "X_red = X.drop(columns=red_features) ## dropping the highly correlated columns\n",
    "\n",
    "## checking whether the high correlations are gone\n",
    "X_red_corr = X_red.corr()\n",
    "X_red_corr = X_red_corr.unstack().reset_index() # Unstack correlation matrix \n",
    "X_red_corr.columns = ['feature_1','feature_2', 'correlation_all'] # rename columns\n",
    "X_red_corr.sort_values(by=\"correlation_all\",ascending=False, inplace=True) # sort by correlation\n",
    "X_red_corr = X_red_corr[X_red_corr['feature_1'] != X_red_corr['feature_2']] # Remove self correlation\n",
    "X_red_corr = X_red_corr.drop_duplicates(subset='correlation_all')\n",
    "\n",
    "\n",
    "## drop columns with nans if in both groups > 80% nans\n",
    "\n",
    "nan_threshold= 0.8 ## adjust the hardcoded values\n",
    "def_nans = def_df.isna().sum()/len(def_df) \n",
    "def_nans_80 = def_nans[def_nans >= nan_threshold].index\n",
    "pay_nans = pay_df.isna().sum()/len(pay_df)\n",
    "pay_nans_80 = pay_nans[pay_nans>= nan_threshold].index\n",
    "nans_80 = [feature for feature in pay_nans_80 if feature in def_nans_80]\n",
    "\n",
    "## check whether features were already removed\n",
    "red_features_nan = [feature for feature in nans_80 if feature not in red_features] \n",
    "X_red = X_red.drop(columns=red_features_nan)\n",
    "dropped_columns = red_features + red_features_nan\n",
    "\n",
    "## Builsing the pipeline\n",
    "num_vars = [feature for feature in X_red.columns[2:] if feature not in cat_vars] ## exclude dates and IDs (first two columns)\n",
    "str_vars = [feature for feature in X_red.columns[2:] if not pd.api.types.is_numeric_dtype(X_red[feature])] ## columns that are not numeric at all \n",
    "#red_cat_vars = [feature for feature in cat_vars if feature not in dropped_columns + str_vars] ## remaining categorical variables that have no string values\n",
    "red_cat_vars = [feature for feature in cat_vars if feature not in dropped_columns + str_vars] ## remaining categorical variables \n",
    "\n",
    "\n",
    "# impute mean/most frequent value for other nans (specific to group?)\n",
    "# robustscale all numerical values\n",
    "\n",
    "num_imputer = SimpleImputer(strategy='mean')\n",
    "num_scaler = RobustScaler()\n",
    "\n",
    "#num_imputer = KNNImputer(n_neighbors=2) ## KNNIMputer is computationally demanding\n",
    "## should come AFTER SCALING\n",
    "\n",
    "num_pipe = make_pipeline(num_imputer, num_scaler)\n",
    "\n",
    "#str_trans = OrdinalEncoder() # is only needed if one wants to do knnimputer\n",
    "\n",
    "#nan_trans = FunctionTransformer(nan_imp)\n",
    "\n",
    "#nan_trans = FunctionTransformer(lambda X: X.applymap(lambda x: np.nan if x in [-1,-1.0, \"-1.0\", \"-1\"] else x))\n",
    "\n",
    "cat_imputer = SimpleImputer(strategy=\"most_frequent\") ## replace with KNNimputer on one neighbour, after transforming to numericals\n",
    "#cat_imputer = KNNImputer(n_neighbors=1) # introducing it did not improve performance, but is computationally demanding\n",
    "cat_encoder = OneHotEncoder(sparse=False, handle_unknown='ignore') ## what happens to the old columns?\n",
    "cat_pipe = make_pipeline(cat_imputer, cat_encoder)\n",
    "#str_pipe = make_pipeline(cat_imputer, str_trans, cat_encoder)\n",
    "#str_pipe = make_pipeline(cat_imputer, cat_encoder)\n",
    "# impute mean/most frequent value for other nans (specific to group?)\n",
    "# robustscale all numerical values\n",
    "\n",
    "num_imputer = SimpleImputer(strategy='mean')\n",
    "num_scaler = RobustScaler()\n",
    "\n",
    "#num_imputer = KNNImputer(n_neighbors=2) ## KNNIMputer is computationally demanding\n",
    "## should come AFTER SCALING\n",
    "\n",
    "num_pipe = make_pipeline(num_imputer, num_scaler)\n",
    "\n",
    "#str_trans = OrdinalEncoder() # is only needed if one wants to do knnimputer\n",
    "\n",
    "#nan_trans = FunctionTransformer(nan_imp)\n",
    "\n",
    "#nan_trans = FunctionTransformer(lambda X: X.applymap(lambda x: np.nan if x in [-1,-1.0, \"-1.0\", \"-1\"] else x))\n",
    "\n",
    "cat_imputer = SimpleImputer(strategy=\"most_frequent\") ## replace with KNNimputer on one neighbour, after transforming to numericals\n",
    "#cat_imputer = KNNImputer(n_neighbors=1) # introducing it did not improve performance, but is computationally demanding\n",
    "cat_encoder = OneHotEncoder(sparse=False, handle_unknown='ignore') ## what happens to the old columns?\n",
    "cat_pipe = make_pipeline(cat_imputer, cat_encoder)\n",
    "#str_pipe = make_pipeline(cat_imputer, str_trans, cat_encoder)\n",
    "#str_pipe = make_pipeline(cat_imputer, cat_encoder)\n",
    "\n",
    "print('done ✅')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c944d6b",
   "metadata": {},
   "source": [
    "# A self-contained alternative Nans imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "333eeb60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done ✅\n"
     ]
    }
   ],
   "source": [
    "def alt_nan_imp(X):\n",
    "    \n",
    "    cat_vars = ['B_30', \n",
    "            'B_38', \n",
    "            'D_114', \n",
    "            'D_116', \n",
    "            'D_117', \n",
    "            'D_120', \n",
    "            'D_126', \n",
    "            'D_63', \n",
    "            'D_64', \n",
    "            'D_66', \n",
    "            'D_68']\n",
    "    \n",
    "    alt_nan_list = [-1,-1.0, \"-1.0\", \"-1\"]\n",
    "    \n",
    "    cat_columns = [column for column in X.columns if column in cat_vars]\n",
    "    \n",
    "    X[cat_columns] = X[cat_columns].applymap(lambda x: np.nan if x in alt_nan_list else x)\n",
    "\n",
    "alt_nan_imp(X_red)\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num_pip', num_pipe, num_vars),\n",
    "    ('cat_pip', cat_pipe, red_cat_vars)],\n",
    "    remainder='drop' ## all columns not in num_vars and red_cat_vars are dropped.\n",
    ")\n",
    "\n",
    "alt_nan_imp(X)\n",
    "preprocessor.fit(X)\n",
    "\n",
    "X_pp = pd.DataFrame(preprocessor.fit_transform(X_red))\n",
    "X_pp['customer_ID'] = X_red['customer_ID']\n",
    "X_pp_avg = X_pp.groupby('customer_ID').mean()\n",
    "y_ID = pd.DataFrame(y)\n",
    "y_ID['customer_ID'] = X_red['customer_ID']\n",
    "y_unique = y_ID.groupby('customer_ID').mean().astype(int) ## actually, this data is just in train_labels\n",
    "X_pp_avg_train, X_pp_avg_val, y_unique_train, y_unique_val = train_test_split(X_pp_avg, y_unique, test_size=0.3) \n",
    "\n",
    "\n",
    "if 'customer_ID' in X_pp.columns:\n",
    "    X_pp.drop(columns='customer_ID', inplace=True)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "# # Train_test_split needs to be on preprocessed data\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "\n",
    "# pp_pred_pipe = make_pipeline(preprocessor, mod)\n",
    "# pp_pred_pipe\n",
    "# pp_pred_pipe.fit(X, y)\n",
    "\n",
    "print('done ✅')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc89b422",
   "metadata": {},
   "source": [
    "# Creating custom amex scoring metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58715c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done ✅\n"
     ]
    }
   ],
   "source": [
    "def amex_metric(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "    \n",
    "    ## TWEAK\n",
    "    y_true = pd.DataFrame(y_true.reset_index(drop=True))\n",
    "    y_pred = pd.DataFrame(y_pred)\n",
    "    \n",
    "    y_true = y_true.rename(columns={y_true.columns[0]:'target'})\n",
    "    y_pred = y_pred.rename(columns={y_pred.columns[0]:'prediction'})\n",
    "    ##\n",
    "    \n",
    "    def top_four_percent_captured(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "        df = (pd.concat([y_true, y_pred], axis='columns')\n",
    "              .sort_values('prediction', ascending=False))\n",
    "        df['weight'] = df['target'].apply(lambda x: 20 if x==0 else 1)\n",
    "        four_pct_cutoff = int(0.04 * df['weight'].sum())\n",
    "        df['weight_cumsum'] = df['weight'].cumsum()\n",
    "        df_cutoff = df.loc[df['weight_cumsum'] <= four_pct_cutoff]\n",
    "        return (df_cutoff['target'] == 1).sum() / (df['target'] == 1).sum()\n",
    "    def weighted_gini(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "        df = (pd.concat([y_true, y_pred], axis='columns')\n",
    "              .sort_values('prediction', ascending=False))\n",
    "        df['weight'] = df['target'].apply(lambda x: 20 if x==0 else 1)\n",
    "        df['random'] = (df['weight'] / df['weight'].sum()).cumsum()\n",
    "        total_pos = (df['target'] * df['weight']).sum()\n",
    "        df['cum_pos_found'] = (df['target'] * df['weight']).cumsum()\n",
    "        df['lorentz'] = df['cum_pos_found'] / total_pos\n",
    "        df['gini'] = (df['lorentz'] - df['random']) * df['weight']\n",
    "        return df['gini'].sum()\n",
    "\n",
    "    def normalized_weighted_gini(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "        y_true_pred = y_true.rename(columns={'target': 'prediction'})\n",
    "        return weighted_gini(y_true, y_pred) / weighted_gini(y_true, y_true_pred)\n",
    "\n",
    "    g = normalized_weighted_gini(y_true, y_pred)\n",
    "    d = top_four_percent_captured(y_true, y_pred)\n",
    "\n",
    "    return 0.5 * (g + d)\n",
    "\n",
    "print('done ✅')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7998583a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done ✅\n"
     ]
    }
   ],
   "source": [
    "amex_metric_scorer = make_scorer(amex_metric)\n",
    "\n",
    "## dict of scoring metrics one might want to pass into cross validation\n",
    "scorings = {'recall':'recall',\n",
    "            'f1':'f1',\n",
    "           'amex': amex_metric_scorer}\n",
    "\n",
    "print('done ✅')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1d3e69",
   "metadata": {},
   "source": [
    "# Averaging preprocessed X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2caf0d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done ✅\n"
     ]
    }
   ],
   "source": [
    "X_pp['customer_ID'] = X_red['customer_ID']\n",
    "X_avg_pp = X_pp.groupby('customer_ID').mean()\n",
    "y_ID = pd.DataFrame(y)\n",
    "y_ID['customer_ID'] = X_red['customer_ID']\n",
    "y_unique = y_ID.groupby('customer_ID').mean().astype(int) ## actually, this data is just in train_labels\n",
    "\n",
    "print('done ✅')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3132c42f",
   "metadata": {},
   "source": [
    "# XG Boost & Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "411bb311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Version 2.0.0-dev\n",
      "done ✅\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV, KFold \n",
    "import xgboost as xgb\n",
    "print('XGB Version',xgb.__version__)\n",
    "\n",
    "print('done ✅')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a261b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "xgb_class = XGBClassifier()\n",
    "\n",
    "# Hyperparameter Grid\n",
    "grid = {'max_depth': [3, 5, 7], \n",
    "        'n_estimators': [88, 90, 92],\n",
    "        'learning_rate': [0.09, 0.1, 0.11]\n",
    "         }\n",
    "\n",
    "# Instanciate Grid Search\n",
    "search = GridSearchCV(xgb_class, \n",
    "                      grid, \n",
    "#                       scoring = 'r2',\n",
    "                      cv = 5,\n",
    "                      n_jobs=-1 # paralellize computation\n",
    "                     ) \n",
    "\n",
    "\n",
    "search.fit(X_pp_avg_train, y_unique_train,\n",
    "    # evaluate loss at each iteration\n",
    "    eval_set=[(X_pp_avg_train,y_unique_train), (X_pp_avg_val, y_unique_val)],  \n",
    "    # stop iterating when eval loss increases 5 times in a row\n",
    "    early_stopping_rounds=4\n",
    ")\n",
    "\n",
    "print('done ✅')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a158289",
   "metadata": {},
   "outputs": [],
   "source": [
    "search.best_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536c893b",
   "metadata": {},
   "outputs": [],
   "source": [
    "search.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f22254",
   "metadata": {},
   "outputs": [],
   "source": [
    "search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003f4abe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cd2566",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_best_reg = XGBClassifier(max_depth= 5,\n",
    "                             n_estimators= 90, \n",
    "                             learning_rate= 0.1 )\n",
    "\n",
    "xgb_best_reg.fit(X_pp_avg_train, y_unique_train,\n",
    "    # evaluate loss at each iteration\n",
    "    eval_set=[(X_pp_avg_train,y_unique_train), (X_pp_avg_val, y_unique_val)],  \n",
    "    # stop iterating when eval loss increases 5 times in a row\n",
    "    early_stopping_rounds=7\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbf29e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgb_best_reg.predict(X_pp_avg_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf686ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "amex_metric(y_unique_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1132a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERSION NAME FOR SAVED MODEL FILES\n",
    "VER = 1\n",
    "\n",
    "# TRAIN RANDOM SEED\n",
    "SEED = 42\n",
    "\n",
    "# FILL NAN VALUE\n",
    "NAN_VALUE = -127 # will fit in int8\n",
    "\n",
    "# FOLDS PER MODEL\n",
    "FOLDS = 5\n",
    "\n",
    "\n",
    "# XGB MODEL PARAMETERS\n",
    "xgb_parms = { \n",
    "    'max_depth':4, \n",
    "    'learning_rate':0.05, \n",
    "    'subsample':0.8,\n",
    "    'colsample_bytree':0.6, \n",
    "    'eval_metric':'logloss',\n",
    "    'objective':'binary:logistic',\n",
    "    'tree_method':'gpu_hist',\n",
    "    'predictor':'gpu_predictor',\n",
    "    'random_state':SEED\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "667483a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "[09:02:25] /Users/sjoerddewit/code/Yuzhe17/AMEX_default_prediction/xgboost/python-package/build/temp.macosx-12.4-x86_64-3.8/xgboost/src/objective/objective.cc:26: Unknown objective function: `{'max_depth': 4, 'learning_rate': 0.05, 'subsample': 0.8, 'colsample_bytree': 0.6, 'eval_metric': 'logloss', 'objective': 'binary:logistic', 'tree_method': 'gpu_hist', 'predictor': 'gpu_predictor', 'random_state': 42}`\nObjective candidate: survival:aft\nObjective candidate: binary:hinge\nObjective candidate: multi:softmax\nObjective candidate: multi:softprob\nObjective candidate: rank:pairwise\nObjective candidate: rank:ndcg\nObjective candidate: rank:map\nObjective candidate: reg:squarederror\nObjective candidate: reg:squaredlogerror\nObjective candidate: reg:logistic\nObjective candidate: binary:logistic\nObjective candidate: binary:logitraw\nObjective candidate: reg:linear\nObjective candidate: reg:pseudohubererror\nObjective candidate: count:poisson\nObjective candidate: survival:cox\nObjective candidate: reg:gamma\nObjective candidate: reg:tweedie\nObjective candidate: reg:absoluteerror\n\nStack trace:\n  [bt] (0) 1   libxgboost.dylib                    0x00000001543fefc5 dmlc::LogMessageFatal::~LogMessageFatal() + 117\n  [bt] (1) 2   libxgboost.dylib                    0x00000001545445d5 xgboost::ObjFunction::Create(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, xgboost::GenericParameter const*) + 1061\n  [bt] (2) 3   libxgboost.dylib                    0x00000001544ef757 xgboost::LearnerConfiguration::ConfigureObjective(xgboost::LearnerTrainParam const&, std::__1::vector<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::allocator<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > >*) + 1767\n  [bt] (3) 4   libxgboost.dylib                    0x00000001544e447e xgboost::LearnerConfiguration::Configure() + 1070\n  [bt] (4) 5   libxgboost.dylib                    0x00000001543f4146 XGBoosterBoostedRounds + 102\n  [bt] (5) 6   libffi.dylib                        0x00007ff8231f8882 ffi_call_unix64 + 82\n  [bt] (6) 7   ???                                 0x000000030590e150 0x0 + 12978282832\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m xgb_ \u001b[38;5;241m=\u001b[39m XGBClassifier(xgb_parms)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mxgb_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_pp_avg_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_unique_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# evaluate loss at each iteration\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_pp_avg_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_unique_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_pp_avg_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_unique_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# stop iterating when eval loss increases 5 times in a row\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/AMEX_default_prediction/lib/python3.8/site-packages/xgboost/core.py:584\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    583\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 584\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/AMEX_default_prediction/lib/python3.8/site-packages/xgboost/sklearn.py:1487\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1459\u001b[0m (\n\u001b[1;32m   1460\u001b[0m     model,\n\u001b[1;32m   1461\u001b[0m     metric,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[1;32m   1467\u001b[0m )\n\u001b[1;32m   1468\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[1;32m   1469\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[1;32m   1470\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     feature_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_types,\n\u001b[1;32m   1485\u001b[0m )\n\u001b[0;32m-> 1487\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1488\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1490\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1495\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1498\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1499\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callable(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n\u001b[1;32m   1502\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/AMEX_default_prediction/lib/python3.8/site-packages/xgboost/core.py:584\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    583\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 584\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/AMEX_default_prediction/lib/python3.8/site-packages/xgboost/training.py:180\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    168\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    169\u001b[0m         EarlyStopping(rounds\u001b[38;5;241m=\u001b[39mearly_stopping_rounds, maximize\u001b[38;5;241m=\u001b[39mmaximize)\n\u001b[1;32m    170\u001b[0m     )\n\u001b[1;32m    171\u001b[0m cb_container \u001b[38;5;241m=\u001b[39m CallbackContainer(\n\u001b[1;32m    172\u001b[0m     callbacks,\n\u001b[1;32m    173\u001b[0m     metric\u001b[38;5;241m=\u001b[39mmetric_fn,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m     output_margin\u001b[38;5;241m=\u001b[39mcallable(obj) \u001b[38;5;129;01mor\u001b[39;00m metric_fn \u001b[38;5;129;01mis\u001b[39;00m feval,\n\u001b[1;32m    178\u001b[0m )\n\u001b[0;32m--> 180\u001b[0m bst \u001b[38;5;241m=\u001b[39m \u001b[43mcb_container\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbefore_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_iteration, num_boost_round):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/AMEX_default_prediction/lib/python3.8/site-packages/xgboost/callback.py:148\u001b[0m, in \u001b[0;36mCallbackContainer.before_training\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;124;03m'''Function called before training.'''\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[0;32m--> 148\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbefore_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_training should return the model\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_cv:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/AMEX_default_prediction/lib/python3.8/site-packages/xgboost/callback.py:350\u001b[0m, in \u001b[0;36mEarlyStopping.before_training\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbefore_training\u001b[39m(\u001b[38;5;28mself\u001b[39m, model: _Model) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _Model:\n\u001b[0;32m--> 350\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstarting_round \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_boosted_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/AMEX_default_prediction/lib/python3.8/site-packages/xgboost/core.py:2381\u001b[0m, in \u001b[0;36mBooster.num_boosted_rounds\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2379\u001b[0m rounds \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mc_int()\n\u001b[1;32m   2380\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2381\u001b[0m \u001b[43m_check_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterBoostedRounds\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrounds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2382\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rounds\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/AMEX_default_prediction/lib/python3.8/site-packages/xgboost/core.py:250\u001b[0m, in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;124;03m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \n\u001b[1;32m    241\u001b[0m \u001b[38;5;124;03mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;124;03m    return value from API calls\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m XGBoostError(py_str(_LIB\u001b[38;5;241m.\u001b[39mXGBGetLastError()))\n",
      "\u001b[0;31mXGBoostError\u001b[0m: [09:02:25] /Users/sjoerddewit/code/Yuzhe17/AMEX_default_prediction/xgboost/python-package/build/temp.macosx-12.4-x86_64-3.8/xgboost/src/objective/objective.cc:26: Unknown objective function: `{'max_depth': 4, 'learning_rate': 0.05, 'subsample': 0.8, 'colsample_bytree': 0.6, 'eval_metric': 'logloss', 'objective': 'binary:logistic', 'tree_method': 'gpu_hist', 'predictor': 'gpu_predictor', 'random_state': 42}`\nObjective candidate: survival:aft\nObjective candidate: binary:hinge\nObjective candidate: multi:softmax\nObjective candidate: multi:softprob\nObjective candidate: rank:pairwise\nObjective candidate: rank:ndcg\nObjective candidate: rank:map\nObjective candidate: reg:squarederror\nObjective candidate: reg:squaredlogerror\nObjective candidate: reg:logistic\nObjective candidate: binary:logistic\nObjective candidate: binary:logitraw\nObjective candidate: reg:linear\nObjective candidate: reg:pseudohubererror\nObjective candidate: count:poisson\nObjective candidate: survival:cox\nObjective candidate: reg:gamma\nObjective candidate: reg:tweedie\nObjective candidate: reg:absoluteerror\n\nStack trace:\n  [bt] (0) 1   libxgboost.dylib                    0x00000001543fefc5 dmlc::LogMessageFatal::~LogMessageFatal() + 117\n  [bt] (1) 2   libxgboost.dylib                    0x00000001545445d5 xgboost::ObjFunction::Create(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, xgboost::GenericParameter const*) + 1061\n  [bt] (2) 3   libxgboost.dylib                    0x00000001544ef757 xgboost::LearnerConfiguration::ConfigureObjective(xgboost::LearnerTrainParam const&, std::__1::vector<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::allocator<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > >*) + 1767\n  [bt] (3) 4   libxgboost.dylib                    0x00000001544e447e xgboost::LearnerConfiguration::Configure() + 1070\n  [bt] (4) 5   libxgboost.dylib                    0x00000001543f4146 XGBoosterBoostedRounds + 102\n  [bt] (5) 6   libffi.dylib                        0x00007ff8231f8882 ffi_call_unix64 + 82\n  [bt] (6) 7   ???                                 0x000000030590e150 0x0 + 12978282832\n\n"
     ]
    }
   ],
   "source": [
    "xgb_ = XGBClassifier(xgb_parms)\n",
    "\n",
    "xgb_.fit(X_pp_avg_train, y_unique_train,\n",
    "    # evaluate loss at each iteration\n",
    "    eval_set=[(X_pp_avg_train,y_unique_train), (X_pp_avg_val, y_unique_val)],  \n",
    "    # stop iterating when eval loss increases 5 times in a row\n",
    "    early_stopping_rounds=7\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc4c3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgb_best_reg.predict(X_pp_avg_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef7b079",
   "metadata": {},
   "outputs": [],
   "source": [
    "amex_metric(y_unique_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ccb7c6",
   "metadata": {},
   "source": [
    "# XG Boost & Optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f342799e",
   "metadata": {},
   "source": [
    "https://optuna.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59fd9105",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04d829c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b441251",
   "metadata": {},
   "source": [
    "# XG Boost & Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc1d390f",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "dlopen(/Users/sjoerddewit/.pyenv/versions/3.8.12/envs/AMEX_default_prediction/lib/python3.8/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: '/usr/local/opt/libomp/lib/libomp.dylib'\n  Referenced from: '/Users/sjoerddewit/.pyenv/versions/3.8.12/envs/AMEX_default_prediction/lib/python3.8/site-packages/lightgbm/lib_lightgbm.so'\n  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlightgbm\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mlgb\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/AMEX_default_prediction/lib/python3.8/site-packages/lightgbm/__init__.py:8\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03m\"\"\"LightGBM, Light Gradient Boosting Machine.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03mContributors: https://github.com/microsoft/LightGBM/graphs/contributors.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbasic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Booster, Dataset, Sequence, register_logger\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallback\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m early_stopping, log_evaluation, print_evaluation, record_evaluation, reset_parameter\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CVBooster, cv, train\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/AMEX_default_prediction/lib/python3.8/site-packages/lightgbm/basic.py:110\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(lib\u001b[38;5;241m.\u001b[39mLGBM_GetLastError()\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\n\u001b[0;32m--> 110\u001b[0m _LIB \u001b[38;5;241m=\u001b[39m \u001b[43m_load_lib\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m NUMERIC_TYPES \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mbool\u001b[39m)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_safe_call\u001b[39m(ret: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/AMEX_default_prediction/lib/python3.8/site-packages/lightgbm/basic.py:101\u001b[0m, in \u001b[0;36m_load_lib\u001b[0;34m()\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lib_path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m lib \u001b[38;5;241m=\u001b[39m \u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcdll\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLoadLibrary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlib_path\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m lib\u001b[38;5;241m.\u001b[39mLGBM_GetLastError\u001b[38;5;241m.\u001b[39mrestype \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mc_char_p\n\u001b[1;32m    103\u001b[0m callback \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mCFUNCTYPE(\u001b[38;5;28;01mNone\u001b[39;00m, ctypes\u001b[38;5;241m.\u001b[39mc_char_p)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/lib/python3.8/ctypes/__init__.py:451\u001b[0m, in \u001b[0;36mLibraryLoader.LoadLibrary\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mLoadLibrary\u001b[39m(\u001b[38;5;28mself\u001b[39m, name):\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dlltype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/lib/python3.8/ctypes/__init__.py:373\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_FuncPtr \u001b[38;5;241m=\u001b[39m _FuncPtr\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 373\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m \u001b[43m_dlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m handle\n",
      "\u001b[0;31mOSError\u001b[0m: dlopen(/Users/sjoerddewit/.pyenv/versions/3.8.12/envs/AMEX_default_prediction/lib/python3.8/site-packages/lightgbm/lib_lightgbm.so, 0x0006): Library not loaded: '/usr/local/opt/libomp/lib/libomp.dylib'\n  Referenced from: '/Users/sjoerddewit/.pyenv/versions/3.8.12/envs/AMEX_default_prediction/lib/python3.8/site-packages/lightgbm/lib_lightgbm.so'\n  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file)"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cf3adb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
